## Self-Attention
-------------------
The Self-Attention Mechanism is a pivotal innovation in artificial intelligence, revolutionizing how models process and understand sequences of data. This mechanism allows models to weigh the importance of different parts of the input data, effectively capturing dependencies regardless of their distance in the sequence. It has become a cornerstone in many advanced AI systems, particularly in natural language processing.

We are offering comprehensive guides to deepen your understanding:

- **[First Guide](https://github.com/fatnaoui/SimLLM/blob/main/Self-Attention/Coding_Attention_Mecanism_Part_1.pdf)**
- **[Second Guide](https://github.com/fatnaoui/SimLLM/blob/main/Self-Attention/Coding_Attention_Mecanism_Part_2.pdf)**

These guides are designed to provide you with a thorough understanding of Self-Attention, from theory to application, and illustrate why it has become a game-changer in the field of AI.

To grasp the importance and the changes that the self-attention mechanism brings, letâ€™s explore how this concept applies in real world:

- First, a comparison of the performance of a simple document classifier while applying self-attention. The attached code **[Document Classifier](https://github.com/fatnaoui/SimLLM/tree/main/Self-Attention/DocumentClassifier)**
![Self Attention Comparison](https://github.com/fatnaoui/SimLLM/blob/main/images/Self_Attention.png)

