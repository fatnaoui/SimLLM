{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Working with Text Data**"
      ],
      "metadata": {
        "id": "0GcrtEZL7bfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this chapter, we'll learn how to prepare input text for training LLMs by splitting it into word and subword tokens, encoding them into vector representations. We'll explore advanced tokenization schemes like byte pair encoding used in models like GPT. Additionally, we'll implement a sampling and data loading strategy to generate input-output pairs for LLM training."
      ],
      "metadata": {
        "id": "RAxOPNE08Hx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.1 Understanding word embeddings**"
      ],
      "metadata": {
        "id": "GgxmKrXL8NBp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep neural networks, including LLMs, cannot directly process raw text as it is categorical and incompatible with neural network operations. To overcome this, text is represented as continuous-valued vectors through a process called embedding. Embeddings can be generated using specific neural network layers or pretrained models, allowing various data types like text, video, and audio to be processed."
      ],
      "metadata": {
        "id": "pebbPswo7Nrb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.2 Tokenizing** **text**"
      ],
      "metadata": {
        "id": "8qCPm8TnxHCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section covers how we split input text into individual tokens, a required preprocessing step for creating embeddings for an LLM. These tokens are\n",
        "either individual words or special characters, including punctuation\n",
        "characters,"
      ],
      "metadata": {
        "id": "_JcJRM-hwjwq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text we will tokenize for LLM training is a short story by Edith Wharton called The Verdict"
      ],
      "metadata": {
        "id": "7o1jDQsPro-9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JEEXqpE1k3yx",
        "outputId": "5cfa4493-207f-47cf-e9da-2ab9b8aacdf4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 20479\n",
            "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
          ]
        }
      ],
      "source": [
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "# The total number of characters\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "# The first 100 characters of this file for illustration purposes:\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our goal is to tokenize this 20479-character short story into individual words and special characters that we can then turn into embeddings for LLM\n",
        "training."
      ],
      "metadata": {
        "id": "Ps1pZYoDsfRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will develop a simple tokenizer using Python's regular expression library re for illustration purposes."
      ],
      "metadata": {
        "id": "pnqMPgrZs5JC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "text = \"Hello, world. This, is a test.\"\n",
        "result = re.split(r'(\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XbcB4znmr0_",
        "outputId": "c9857910-4c2b-4053-bc71-2d72e1b06571"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello,', ' ', 'world.', ' ', 'This,', ' ', 'is', ' ', 'a', ' ', 'test.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the regular expression splits on whitespaces (\\s) and commas, and periods ([,.])\n",
        "result = re.split(r'([,.]|\\s)', text)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdKAQCewm4qT",
        "outputId": "09db1bd1-7590-4adb-ec63-9a406c44cc28"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', '', ' ', 'world', '.', '', ' ', 'This', ',', '', ' ', 'is', ' ', 'a', ' ', 'test', '.', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove whitespace characters\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdwQ0hahnGyr",
        "outputId": "1cfa1916-4292-41b2-c2f0-44ebedac75e2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'This', ',', 'is', 'a', 'test', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing whitespaces reduces memory and computing needs but may miss important structural information, like in Python code. Initially, we'll remove them for simplicity, but later include them for more accurate tokenization."
      ],
      "metadata": {
        "id": "PWe1wJQ1upoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify it a bit further so that it can also handle other types of punctuation\n",
        "text = \"Hello, world. Is this-- a test?\"\n",
        "result = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
        "result = [item.strip() for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hArfTU5fqMjp",
        "outputId": "7a585193-d7c1-4003-9258-a418c39699ee"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'world', '.', 'Is', 'this', '--', 'a', 'test', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the tokenizer to Edith Wharton's entire short story\n",
        "preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "print(len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr43NgU_qT1A",
        "outputId": "0bbae79a-3655-4763-d8a6-96580fd8d3c1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(preprocessed[:30])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV9R7dMlqdAM",
        "outputId": "de37b601-5e1a-4b77-98ca-b03ab78ce22a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'HAD', 'always', 'thought', 'Jack', 'Gisburn', 'rather', 'a', 'cheap', 'genius', '--', 'though', 'a', 'good', 'fellow', 'enough', '--', 'so', 'it', 'was', 'no', 'great', 'surprise', 'to', 'me', 'to', 'hear', 'that', ',', 'in']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting output shows that our tokenizer appears to be handling the text well since all words and special characters are neatly separated."
      ],
      "metadata": {
        "id": "jjFzvq1WwFoA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.3 Converting tokens into token IDs**"
      ],
      "metadata": {
        "id": "jLuVIGch0Rmd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will convert these tokens from a Python string to an integer representation to produce the so-called token IDs. This conversion is an intermediate step before converting the token IDs into\n",
        "embedding vectors."
      ],
      "metadata": {
        "id": "cN_h1slq0Kna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have to build a so-called vocabulary first. This vocabulary defines how we map each unique word and special character to a unique integer"
      ],
      "metadata": {
        "id": "EhONqvpz4Dz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a list of all unique tokens and sort them alphabetically\n",
        "all_words = sorted(list(set(preprocessed)))\n",
        "vocab_size = len(all_words)\n",
        "# Determine the vocabulary size\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFOmyMXh4Q_b",
        "outputId": "eac6eb3b-a144-45af-81a8-57369447c74c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vocabulary and print its first 50 entries for illustration purposes\n",
        "vocab = {token:integer for integer,token in enumerate(all_words)}\n",
        "for i, item in enumerate(vocab.items()):\n",
        "  print(item)\n",
        "  if i > 50:\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymFjjWxY45t7",
        "outputId": "04b93f86-6f43-47f3-d005-81f0dc632049"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('!', 0)\n",
            "('\"', 1)\n",
            "(\"'\", 2)\n",
            "('(', 3)\n",
            "(')', 4)\n",
            "(',', 5)\n",
            "('--', 6)\n",
            "('.', 7)\n",
            "(':', 8)\n",
            "(';', 9)\n",
            "('?', 10)\n",
            "('A', 11)\n",
            "('Ah', 12)\n",
            "('Among', 13)\n",
            "('And', 14)\n",
            "('Are', 15)\n",
            "('Arrt', 16)\n",
            "('As', 17)\n",
            "('At', 18)\n",
            "('Be', 19)\n",
            "('Begin', 20)\n",
            "('Burlington', 21)\n",
            "('But', 22)\n",
            "('By', 23)\n",
            "('Carlo', 24)\n",
            "('Carlo;', 25)\n",
            "('Chicago', 26)\n",
            "('Claude', 27)\n",
            "('Come', 28)\n",
            "('Croft', 29)\n",
            "('Destroyed', 30)\n",
            "('Devonshire', 31)\n",
            "('Don', 32)\n",
            "('Dubarry', 33)\n",
            "('Emperors', 34)\n",
            "('Florence', 35)\n",
            "('For', 36)\n",
            "('Gallery', 37)\n",
            "('Gideon', 38)\n",
            "('Gisburn', 39)\n",
            "('Gisburns', 40)\n",
            "('Grafton', 41)\n",
            "('Greek', 42)\n",
            "('Grindle', 43)\n",
            "('Grindle:', 44)\n",
            "('Grindles', 45)\n",
            "('HAD', 46)\n",
            "('Had', 47)\n",
            "('Hang', 48)\n",
            "('Has', 49)\n",
            "('He', 50)\n",
            "('Her', 51)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Later, when we want to convert the outputs of an LLM from numbers back into text, we also need a way to turn token IDs into text."
      ],
      "metadata": {
        "id": "neuoWYVz63za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement a complete tokenizer class with an encode and decode method\n",
        "class SimpleTokenizerV1:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Gr5Zmrx469WN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize a passage from Edith Wharton's short story using SimpleTokenizerV1\n",
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "text = \"\"\"\"It's the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\"\"\"\n",
        "ids = tokenizer.encode(text)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrOyHcTG7u3-",
        "outputId": "0c08a4b6-0b57-4d78-883f-f1ade0bcdd5a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 58, 2, 872, 1013, 615, 541, 763, 5, 1155, 608, 5, 1, 69, 7, 39, 873, 1136, 773, 812, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn these token IDs back into text using the decode method:\n",
        "print(tokenizer.decode(ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3K_sN1a--flG",
        "outputId": "e8de1f2d-f3c4-41c6-9446-eb38837d77e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\" It' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We implemented a tokenizer capable of tokenizing and de-tokenizing text based on a snippet from the training set. Let's now apply it to\n",
        "a new text sample that is not contained in the training set"
      ],
      "metadata": {
        "id": "ad5hYlSO-2Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea?\"\n",
        "# tokenizer.encode(text)"
      ],
      "metadata": {
        "id": "dJt8WFD--7q2"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The problem is that the word \"Hello\" was not used in the The Verdict short\n",
        "story. Hence, it is not contained in the vocabulary."
      ],
      "metadata": {
        "id": "reQoAm0h_JMy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.4 Adding special context tokens**"
      ],
      "metadata": {
        "id": "GylarYEWIZ2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will modify this tokenizer\n",
        "to handle unknown words. we will modify the tokenizer to use an <|unk|> token\n",
        "if it encounters a word that is not part of the vocabulary. Furthermore, we will add\n",
        "a <|endoftext|> token between unrelated texts."
      ],
      "metadata": {
        "id": "0HQcQH-_IgGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Modify the vocabulary to include these two special tokens, <unk> and <|endoftext|>\n",
        "all_tokens = sorted(list(set(preprocessed)))\n",
        "all_tokens.extend([\"<|endoftext|>\", \"<|unk|>\"])\n",
        "vocab = {token:integer for integer,token in enumerate(all_tokens)}\n",
        "print(len(vocab.items()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRtWh45NPwBA",
        "outputId": "1923cf35-0748-4d08-cfe9-231b642ba4c0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the last 5 entries of the updated vocabulary\n",
        "for i, item in enumerate(list(vocab.items())[-5:]):\n",
        "  print(item)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sgI6VBdQpmt",
        "outputId": "4dbe384f-a138-43a0-fd48-df450adf7978"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('younger', 1156)\n",
            "('your', 1157)\n",
            "('yourself', 1158)\n",
            "('<|endoftext|>', 1159)\n",
            "('<|unk|>', 1160)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple text tokenizer that handles unknown words:It replaces unknown words by <|unk|> tokens\n",
        "class SimpleTokenizerV2:\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = { i:s for s,i in vocab.items()}\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [item if item in self.str_to_int\n",
        "    else \"<|unk|>\" for item in preprocessed]\n",
        "    ids = [self.str_to_int[s] for s in preprocessed]\n",
        "    return ids\n",
        "  def decode(self, ids):\n",
        "    text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "    text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "oEWpLuTJQ4id"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use a simple text sample concatenated from two independent and unrelated sentences\n",
        "text1 = \"Hello, do you like tea?\"\n",
        "text2 = \"In the sunlit terraces of the palace.\"\n",
        "text = \" <|endoftext|> \".join((text1, text2))\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj190X7CRpfm",
        "outputId": "f0bba95f-c90d-44ea-eb87-81a6c5ef6c79"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of the palace.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Try the new tokenizer\n",
        "tokenizer = SimpleTokenizerV2(vocab)\n",
        "print(tokenizer.encode(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hfHw_TuR_nV",
        "outputId": "fca78b39-3fbf-40a4-8dad-eccba0bf92f5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1160, 5, 362, 1155, 642, 1000, 10, 1159, 57, 1013, 981, 1009, 738, 1013, 1160, 7]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that the list of token IDs contains 1159 for the <|endoftext|> separator token as well as two 1160 tokens, which are used for\n",
        "unknown words."
      ],
      "metadata": {
        "id": "ZEwHXpkPTnl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(tokenizer.encode(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEX8UTFSSRld",
        "outputId": "ce9d8307-ebd2-492e-c9d6-1659c7cbfd19"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|unk|>, do you like tea? <|endoftext|> In the sunlit terraces of the <|unk|>.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenizer used for GPT models also doesn't use an <|unk|>\n",
        "token for out-of-vocabulary words. Instead, GPT models use a byte pair\n",
        "encoding tokenizer, which breaks down words into subword units"
      ],
      "metadata": {
        "id": "R0-n_2qpT4KC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.5 Byte pair encoding**"
      ],
      "metadata": {
        "id": "VKJE5VGMT6vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since implementing BPE can be relatively complicated, we will use an\n",
        "existing Python open-source library called tiktoken"
      ],
      "metadata": {
        "id": "WHunstEx2NCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "# check the version\n",
        "from importlib.metadata import version\n",
        "import tiktoken\n",
        "print(\"tiktoken version:\", version(\"tiktoken\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebv2J-pbU6Su",
        "outputId": "4488e7cf-f711-4842-a89d-b340a8ead359"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.1 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n",
            "tiktoken version: 0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate the BPE tokenizer from tiktoken\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")"
      ],
      "metadata": {
        "id": "xCThEIpXVx1M"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace \"\n",
        "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
        "print(integers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--R8TyVYV_DM",
        "outputId": "4e0f8116-634f-4635-c91f-5c7616cd9708"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 286, 617, 34680, 27271, 220]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "strings = tokenizer.decode(integers)\n",
        "print(strings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvktWgh5WhW5",
        "outputId": "0aaba71e-0f65-496f-e6a4-7ec80d3097cc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "observations:\n",
        "* The token `<|endoftext|>` is assigned the largest token ID, 50256. The BPE tokenizer used for models like GPT-2, GPT-3, and the original ChatGPT,has a total vocabulary size of 50,257.\n",
        "* the BPE tokenizer above encodes and decodes unknown words, such\n",
        "as \"someunknownPlace\" correctly"
      ],
      "metadata": {
        "id": "e0ru8vfP3yd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.6 Data sampling with a sliding window**"
      ],
      "metadata": {
        "id": "CBPSzzoc8Fc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step before we\n",
        "can finally create the embeddings for the LLM is to generate the input-target\n",
        "pairs required for training an LLM.\n",
        "In this section we implement a data loader that fetches the input-target pairs from the training dataset using a sliding window\n",
        "approach."
      ],
      "metadata": {
        "id": "PoNg95nX-OFz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the whole The Verdict short story using the BPE tokenizer\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "  enc_text = tokenizer.encode(raw_text)\n",
        "  print(len(enc_text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTmU1jAJ2lIu",
        "outputId": "66d4b0f8-f6fb-4ab3-fa7c-12b302ea2dea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove the first 50 tokens from the dataset for demonstration purposes\n",
        "enc_sample = enc_text[50:]"
      ],
      "metadata": {
        "id": "ZVwcBzYF_S9S"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the easiest and most intuitive ways to create the input-target pairs for\n",
        "the next-word prediction task is to create two variables, x and y, where x\n",
        "contains the input tokens and y contains the targets, which are the inputs\n",
        "shifted by 1"
      ],
      "metadata": {
        "id": "kTK9J3d4BQ7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_size = 4\n",
        "x = enc_sample[:context_size]\n",
        "y = enc_sample[1:context_size+1]\n",
        "print(f\"x: {x}\")\n",
        "print(f\"y:     {y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apFu0I1kAMFO",
        "outputId": "eb97f63a-36e7-49b0-b931-95427ae3c467"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x: [290, 4920, 2241, 287]\n",
            "y:     [4920, 2241, 287, 257]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(context, \"---->\", desired)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNKd0lbYBjza",
        "outputId": "62473eaa-8388-451e-ec29-3042ed2e2d1d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[290] ----> 4920\n",
            "[290, 4920] ----> 2241\n",
            "[290, 4920, 2241] ----> 287\n",
            "[290, 4920, 2241, 287] ----> 257\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For illustration purposes, let's repeat the previous code but convert the token IDs into text\n",
        "for i in range(1, context_size+1):\n",
        "    context = enc_sample[:i]\n",
        "    desired = enc_sample[i]\n",
        "    print(tokenizer.decode(context), \"---->\", tokenizer.decode([desired]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wzkb4t2BzFv",
        "outputId": "57ff7f31-4014-4922-d6c1-c5571b72ea59"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " and ---->  established\n",
            " and established ---->  himself\n",
            " and established himself ---->  in\n",
            " and established himself in ---->  a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For the efficient data loader implementation, we will use PyTorch's built-in Dataset and DataLoader classes\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self, txt, tokenizer, max_length, stride):\n",
        "    self.tokenizer = tokenizer\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    token_ids = tokenizer.encode(txt)\n",
        "    for i in range(0, len(token_ids) - max_length, stride):\n",
        "      input_chunk = token_ids[i:i + max_length]\n",
        "      target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "Xgbs8e0fB-WP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the GPTDatasetV1 to load the inputs in batches via a PyTorch DataLoader\n",
        "def create_dataloader_v1(txt, batch_size=4,\n",
        "  max_length=256, stride=128, shuffle=True, drop_last=True):\n",
        "  tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "  dataloader = DataLoader(\n",
        "  dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "h7wYCYmTGMf-"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Test the dataloader\n",
        "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "  dataloader = create_dataloader_v1(\n",
        "  raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
        "  data_iter = iter(dataloader)\n",
        "  first_batch = next(data_iter)\n",
        "  print(first_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZVh-pyNGtqa",
        "outputId": "d81e8049-2c51-4bb5-df27-41a449082334"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first_batch variable contains two tensors: the first tensor stores the\n",
        "input token IDs, and the second tensor stores the target token IDs."
      ],
      "metadata": {
        "id": "-Yvrp4jHIr54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To illustrate the meaning of stride=1\n",
        "second_batch = next(data_iter)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv2llOdjItHI",
        "outputId": "72dbcb48-f0ba-48f3-842d-bce7ddca1f3c"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[ 367, 2885, 1464, 1807]]), tensor([[2885, 1464, 1807, 3619]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "we can see that the second\n",
        "batch's token IDs are shifted by one position compared to the first batch.\n",
        "\n",
        "If the stride is set to 1, we shift the input window by 1 position when creating the\n",
        "next batch. If we set the stride equal to the input window size, we can prevent overlaps between\n",
        "the batches."
      ],
      "metadata": {
        "id": "ZX1CN03EJSnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size greater than 1\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJvWQIpmK4CL",
        "outputId": "67c17dde-b7dc-4c33-ba9b-6704a1c0995a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Targets:\n",
            " tensor([[  367,  2885,  1464,  1807],\n",
            "        [ 3619,   402,   271, 10899],\n",
            "        [ 2138,   257,  7026, 15632],\n",
            "        [  438,  2016,   257,   922],\n",
            "        [ 5891,  1576,   438,   568],\n",
            "        [  340,   373,   645,  1049],\n",
            "        [ 5975,   284,   502,   284],\n",
            "        [ 3285,   326,    11,   287]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.7 Creating token embeddings**"
      ],
      "metadata": {
        "id": "QrkN6JjEtrdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step for preparing the input text for LLM training is to convert the\n",
        "token IDs into embedding vectors\n",
        "\n",
        "Note\n",
        "that we initialize these embedding weights with random values as a\n",
        "preliminary step"
      ],
      "metadata": {
        "id": "fTHjJqsRtuta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate an embedding layer in PyTorch\n",
        "input_ids = torch.tensor([2, 3, 5, 1])\n",
        "vocab_size = 6\n",
        "output_dim = 3\n",
        "torch.manual_seed(123)\n",
        "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "print(embedding_layer.weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1lFOAYdvef0",
        "outputId": "70a77632-b324-4a77-921c-88f9a0e40448"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Parameter containing:\n",
            "tensor([[ 0.3374, -0.1778, -0.1690],\n",
            "        [ 0.9178,  1.5810,  1.3010],\n",
            "        [ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-1.1589,  0.3255, -0.6315],\n",
            "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_layer(input_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxkngLG4wFIH",
        "outputId": "2c158433-819c-4c33-f431-281ca756c8a3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.2753, -0.2010, -0.1606],\n",
            "        [-0.4015,  0.9666, -1.1481],\n",
            "        [-2.8400, -0.7849, -1.4096],\n",
            "        [ 0.9178,  1.5810,  1.3010]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation: Each row in this output matrix is obtained via a lookup operation from the\n",
        "embedding weight matrix,"
      ],
      "metadata": {
        "id": "DwCRbHkyxx1h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.8 Encoding word positions**"
      ],
      "metadata": {
        "id": "dXLqrUWgx6Ze"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The way the previously introduced embedding layer works is that the same\n",
        "token ID always gets mapped to the same vector representation, regardless of\n",
        "where the token ID is positioned in the input sequence"
      ],
      "metadata": {
        "id": "c4Qe4jVo4RTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "there are two broad categories of position-aware\n",
        "embeddings: relative positional embeddings and absolute positional\n",
        "embeddings.\n",
        "* Absolute positional embeddings are directly associated with specific\n",
        "positions in a sequence.\n",
        "*the emphasis of\n",
        "relative positional embeddings is on the relative position or distance between\n",
        "tokens. This means the model learns the relationships in terms of \"how far\n",
        "apart\" rather than \"at which exact position.\""
      ],
      "metadata": {
        "id": "nYLPqRsN-rZ5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We now consider more realistic and useful embedding sizes and encode the input tokens into a 256-dimensional vector representation.\n",
        "output_dim = 256\n",
        "vocab_size = 50257\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)"
      ],
      "metadata": {
        "id": "XrjnXuml_Xno"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 4\n",
        "dataloader = create_dataloader_v1(\n",
        "raw_text, batch_size=8, max_length=max_length, stride=max_length, shuffle=False)\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Token IDs:\\n\", inputs)\n",
        "print(\"\\nInputs shape:\\n\", inputs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fUHyQfGAQOx",
        "outputId": "399d20de-05d1-4a87-ebcb-8e1013e794c6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs:\n",
            " tensor([[   40,   367,  2885,  1464],\n",
            "        [ 1807,  3619,   402,   271],\n",
            "        [10899,  2138,   257,  7026],\n",
            "        [15632,   438,  2016,   257],\n",
            "        [  922,  5891,  1576,   438],\n",
            "        [  568,   340,   373,   645],\n",
            "        [ 1049,  5975,   284,   502],\n",
            "        [  284,  3285,   326,    11]])\n",
            "\n",
            "Inputs shape:\n",
            " torch.Size([8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, the token ID tensor is 8x4-dimensional, meaning that the data\n",
        "batch consists of 8 text samples with 4 tokens each"
      ],
      "metadata": {
        "id": "KSSXu7EhAkGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the embedding layer to embed these token IDs into 256-dimensional vectors\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IprfuNM7Alma",
        "outputId": "22607794-0293-43fb-e2c5-56a606c1678e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a GPT model's absolute embedding approach, we just need to create\n",
        "another embedding layer that has the same dimension as the\n",
        "token_embedding_layer:"
      ],
      "metadata": {
        "id": "eHJ423uwBvqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = max_length\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoHUirMEBzZ7",
        "outputId": "d1d24877-9dd1-4fad-d6b1-0d02c7587064"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_embeddings = token_embeddings + pos_embeddings\n",
        "print(input_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCk-CdgACJOX",
        "outputId": "3f6853cd-c1d1-4414-e25e-828b3565611e"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 4, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The input_embeddings we created are the\n",
        "embedded input examples that can now be processed by the main LLM\n",
        "modules."
      ],
      "metadata": {
        "id": "9K-vThVf6ed3"
      }
    }
  ]
}